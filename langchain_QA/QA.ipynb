{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f1fb16-6a0e-42f7-8bfb-ef70ed372011",
   "metadata": {},
   "source": [
    "# 本地模型部署(chatGLM 6b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3ce7b-7c5e-454b-8296-934817703f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    源自清华大学 github 网站上的案例（https://github.com/THUDM/ChatGLM-6B）\n",
    "\n",
    "    调试量化 int4级别\n",
    "\n",
    "    先保证本地安装调试完CUDA，然后创建conda 环境 进行环境配置 pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple\n",
    "\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = \"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device='cuda')\n",
    "# 按需修改，目前只支持 4/8 bit 量化\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response + '\\n')\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca471120-9a97-4de4-896a-753e89b83474",
   "metadata": {},
   "source": [
    "# 上面这种模型 定义 是不能够放入到 `RetrievalQA` 中的，因为这里的 `model` 是 `AutoModel` 类型，并不是可以 `Runnable` 的\n",
    "\n",
    "**这样部署的模型直接拿去用是不对的，会出现下面错误：**\n",
    "\n",
    "```python  \n",
    "ValidationError: 2 validation errors for LLMChain\n",
    "\n",
    "\n",
    "llm\n",
    "  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n",
    "llm\n",
    "  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6985e23-a7c6-4800-ab55-cf8bf97662a6",
   "metadata": {},
   "source": [
    "# 正确的 `model` 定义如下：(这个还有点小问题，后面研究)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f3e72f-1aa0-4509-9f6e-6d942f249195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:14:27.001926100Z",
     "start_time": "2024-06-04T01:14:24.777361400Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class HuggingfaceModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = self.model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface\"\n",
    "\n",
    "    def generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> List[str]:\n",
    "        callback_manager = CallbackManagerForLLMRun.get_instance()\n",
    "        with callback_manager.as_context():\n",
    "            callback_manager.on_llm_start({self._llm_type: len(prompts)})\n",
    "            results = [self._call(prompt, stop) for prompt in prompts]\n",
    "            callback_manager.on_llm_end({self._llm_type: len(prompts)})\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3bd31-bd9d-4189-8213-a9b64bdf4ffa",
   "metadata": {},
   "source": [
    "# 下面这个是没问题的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c424c9-a704-42cf-bc82-64a1630c0b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:18:38.010439900Z",
     "start_time": "2024-06-04T01:18:36.599754700Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "class ChatGLM_LLM(LLM):\n",
    "    # 基于本地 InternLM 自定义 LLM 类\n",
    "    tokenizer : AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: InternLM 模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(torch.bfloat16).cuda()\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        # 重写调用函数\n",
    "        response, history = self.model.chat(self.tokenizer, prompt , history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM3-6B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731b06c-7cf1-4f57-8514-7e69d89aa97b",
   "metadata": {},
   "source": [
    "# 本地知识库搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6233126-4a05-4691-b5a1-8ff92b8da4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "# 加载文件夹中的所有.md类型的文件\n",
    "loader = DirectoryLoader(r'D:/Notes/NLP review', glob='**/*.md')\n",
    "# 将数据转成 document 对象，每个文件会作为一个 document\n",
    "documents = loader.load()\n",
    "\n",
    "# 初始化加载器\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "# 切割加载的 document\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 指定 Hugging Face 的预训练模型名称\n",
    "model_name = r\"D:\\CodeLibrary\\ChatGLM\\embedtext2vec\"  # 示例模型名称，您可以根据需要更改\n",
    "\n",
    "# 创建 HuggingFaceEmbeddings 对象\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# 构建持久化的 向量数据库    \n",
    "# vertorstore 持久化地址\n",
    "persist_directory = r'D:\\CodeLibrary\\ChatGLM\\langchain_QA\\chroma_data'\n",
    "\n",
    "#将数据转化为 embeddings 存入chroma,并设置本地磁盘持久化路径\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# 持久化\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b244104-b7ef-49ea-a2fa-ea876eaf6987",
   "metadata": {},
   "source": [
    "# langchain集成                 \n",
    "1. 直接导入之前持久化的向量数据库，无需重新构建           \n",
    "2. 定义直接重写LLM接口的llm          \n",
    "3. 通过 langchain 提供的 `RetirevalQA` 对象，构建RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4716c927-9cf6-4cb7-b163-e35adce9571d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:18:51.354487500Z",
     "start_time": "2024-06-04T01:18:47.581936500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d6872d18754eef8f9389764a119aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. 将 document 通过 HuggingFace 的 embeddings 模型计算 embedding 向量信息并临时存入 Chroma 向量数据库，用于后续匹配查询\n",
    "\n",
    "2. 后面在 RetrievalQA.from_chain_type 中, 使用 retriever=docsearch.as_retriever()\n",
    "\n",
    "'''\n",
    "# docsearch = Chroma.from_documents(split_docs, embeddings)\n",
    "\n",
    "\n",
    "# 加载 向量数据库           \n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 初始化自定义 llm \n",
    "model_name = r\"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "\n",
    "chatglm2 = ChatGLM_LLM(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211d00f-c420-44a1-9201-628f9f22cbed",
   "metadata": {},
   "source": [
    "# 检索问答链 参考代码\n",
    "```python\n",
    "store = Chroma.load(\"chroma_store\", embeddings)\n",
    "\n",
    "template = \"\"\"基于以下信息来回答用户问题。                          \n",
    "已知信息：   \n",
    "{context} \n",
    "问题：\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "qa = RetrievalQA.from_chain_type(llm=chatglm, retriever=store.as_retriever(), chain_type=\"stuff\",\n",
    "                                chain_type_kwargs=chain_type_kwargs, return_source_documents=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49d0206-57fd-4625-be01-ff917e7ed3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '什么是RAG？', 'result': 'RAG（Reinforcement-Adversarial Generation）是一种用于自然语言处理和计算机视觉任务的生成模型，结合了强化学习和对抗学习的方法。它通过在生成任务中使用对抗策略，使得生成模型的训练更加高效和可控。\\n\\n在RAG中，生成器（Generator）和判别器（Discriminator）是两个相互对抗的神经网络，生成器试图生成有用的数据，而判别器则试图区分真实数据和生成数据。两个网络在训练过程中不断进行交互，生成器通过向判别器提供真实数据来更新网络参数，而判别器通过判断生成器生成的数据是否真实来更新网络参数。\\n\\nRAG的应用范围非常广泛，包括图像生成、文本生成、语音合成等任务。它能够生成高质量、多样化的数据，同时还能够提高模型的可扩展性和鲁棒性。', 'source_documents': [Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "template = '''\n",
    "基于以下信息来回答用户问题。如果你不知道答案，就说你不知道，不要试图编造答案。尽量使答案简单，并最后回答的最后说“谢谢你的提问！”。                      \n",
    "已知信息： \n",
    "{context} \n",
    "问题：\n",
    "{question}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "\n",
    "# 创建问答对象\n",
    "# qa = RetrievalQA.from_chain_type(llm=hf_model, chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chatglm2,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 进行问答 Chain\n",
    "\"\"\"\n",
    "    类的 __call__ 方法已经被弃用， 用 invoke 方法来代替 __call__ 方法\n",
    "\"\"\"\n",
    "result = qa.invoke({\"query\": \"什么是RAG？\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b6ea12-4847-4147-a737-1b8940a978ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query', 'result', 'source_documents'])\n"
     ]
    }
   ],
   "source": [
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b86b01-420d-4a06-9227-097d532af44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是RAG？\n"
     ]
    }
   ],
   "source": [
    "print(result['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1937d4b7-0bef-43e2-af5e-86a3523c47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Reinforcement-Adversarial Generation）是一种用于自然语言处理和计算机视觉任务的生成模型，结合了强化学习和对抗学习的方法。它通过在生成任务中使用对抗策略，使得生成模型的训练更加高效和可控。\n",
      "\n",
      "在RAG中，生成器（Generator）和判别器（Discriminator）是两个相互对抗的神经网络，生成器试图生成有用的数据，而判别器则试图区分真实数据和生成数据。两个网络在训练过程中不断进行交互，生成器通过向判别器提供真实数据来更新网络参数，而判别器通过判断生成器生成的数据是否真实来更新网络参数。\n",
      "\n",
      "RAG的应用范围非常广泛，包括图像生成、文本生成、语音合成等任务。它能够生成高质量、多样化的数据，同时还能够提高模型的可扩展性和鲁棒性。\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752939f6-515e-4052-a49a-ab3d487a4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'})]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9cc4c83-2570-46f5-8d38-664c3131f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @classmethod\n",
      "    def from_chain_type(\n",
      "        cls,\n",
      "        llm: BaseLanguageModel,\n",
      "        chain_type: str = \"stuff\",\n",
      "        chain_type_kwargs: Optional[dict] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> BaseRetrievalQA:\n",
      "        \"\"\"Load chain from chain type.\"\"\"\n",
      "        _chain_type_kwargs = chain_type_kwargs or {}\n",
      "        combine_documents_chain = load_qa_chain(\n",
      "            llm, chain_type=chain_type, **_chain_type_kwargs\n",
      "        )\n",
      "        return cls(combine_documents_chain=combine_documents_chain, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(RetrievalQA.from_chain_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3882d8c-5602-48b7-9d03-b30e05800d80",
   "metadata": {},
   "source": [
    "# 看了下知识库中的内容再进行一次查询发现是有效的（之前效果不好可能是因为知识库的原因，以及部署chatglm6b 我选择的精度比较低）,还可能是因为我使用 CharacterTextSplitter 的时候 overlap设置的为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b48d08-90eb-4930-9655-ae133a527bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在搜索相关问题时，我了解到RAG（Region of Interest）是指在文档中提取出的特定主题区域。而RAG 融合是指将来自多个RAG模型的结果进行有效整合，从而提高整体系统的性能。在RAG模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。因此，RAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体系统的性能。\n",
      "\n",
      "具体而言，RAG 融合可以通过以下方式实现：\n",
      "\n",
      "1. 交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\n",
      "\n",
      "2. 信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\n",
      "\n",
      "3. 多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\n",
      "\n",
      "4. 端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\n",
      "\n",
      "最后，RAG 融合可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\n"
     ]
    }
   ],
   "source": [
    "a = qa.invoke({'query':'什么是RAG 融合'})\n",
    "print(a['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21594fa3-9b64-47ab-bdbb-07c7eb22d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'}), Document(page_content='模型管理：Triton提供了模型版本管理和模型仓库功能，使得模型部署和更新更加灵活和方便。\\n\\n多模型服务：Triton支持同时部署多个模型，并能够根据请求动态加载和卸载模型，这对于需要处理多种任务的场景非常有用。\\n\\n自定义后端：Triton允许开发者编写自定义的后端插件，以支持新的模型格式或实现特定的推理逻辑。\\n\\n与PyTorch的关系\\n\\n模型转换：要将PyTorch模型部署到Triton上，通常需要先将模型转换为ONNX格式。ONNX是一个开放的格式，用于表示深度学习模型，它被Triton等推理引擎广泛支持。\\n\\n推理加速：Triton可以利用TensorRT等工具对ONNX模型进行优化，从而在NVIDIA GPU上实现高效的推理。\\n\\n部署和管理：一旦模型转换为ONNX格式，就可以使用Triton来部署和管理这些模型，提供高效的推理服务。\\n\\n总之，Triton是一个强大的推理引擎，它可以与PyTorch等深度学习框架协同工作，帮助开发者高效地部署和运行深度学习模型。\\n\\nTensorRT\\n\\nTensorRT是NVIDIA推出的一款深度学习推理（Inference）引擎，它专门用于优化深度学习模型以实现高性能的推理。TensorRT通过将深度学习模型（如CNN、RNN等）转换为优化的推理引擎，能够在NVIDIA GPU上提供高效的推理性能。\\n\\nTensorRT的主要特点包括：\\n\\n模型优化：TensorRT可以对深度学习模型进行层与层的融合、精度校准、内核自动调优等优化，以提高推理速度和减少内存占用。\\n\\n支持多种框架：TensorRT支持多种深度学习框架，如Caffe、TensorFlow、PyTorch等。用户可以将这些框架训练的模型转换为TensorRT可以理解的格式。\\n\\n多种精度支持：TensorRT支持FP32、FP16、INT8等多种精度，允许用户根据性能和精度需求进行选择。\\n\\n易于集成：TensorRT提供了C++和Python API，可以轻松集成到现有的应用程序中。\\n\\n跨平台支持：TensorRT可以在多种操作系统上运行，包括Windows、Linux等。\\n\\n使用TensorRT的步骤通常包括：\\n\\n模型转换：首先，需要将训练好的模型从原始的深度学习框架（如PyTorch、TensorFlow等）转换为TensorRT可以理解的格式，如ONNX。\\n\\n优化和校准：使用TensorRT对模型进行优化，包括层与层的融合、精度校准等。\\n\\n构建推理引擎：将优化后的模型构建为TensorRT推理引擎。\\n\\n推理：使用构建好的推理引擎进行推理，处理输入数据并生成输出。\\n\\nTensorRT特别适用于需要高性能推理的场景，如自动驾驶、视频分析、语音识别等。通过使用TensorRT，开发者可以在不牺牲太多精度的情况下显著提高模型的推理速度，从而满足实时性要求。\\n\\nTriton 与 TensorRT\\n\\nTriton Inference Server和TensorRT之间有密切的关系。Triton Inference Server是一个开源的推理引擎，由NVIDIA开发，旨在提供高性能、灵活的深度学习模型推理服务。而TensorRT是NVIDIA的一个深度学习推理优化工具，用于优化和加速深度学习模型的推理性能。\\n\\nTriton与TensorRT的关系：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\model deploy and accumulate\\\\model_deploy.md'}), Document(page_content='Triton 与 TensorRT\\n\\nTriton Inference Server和TensorRT之间有密切的关系。Triton Inference Server是一个开源的推理引擎，由NVIDIA开发，旨在提供高性能、灵活的深度学习模型推理服务。而TensorRT是NVIDIA的一个深度学习推理优化工具，用于优化和加速深度学习模型的推理性能。\\n\\nTriton与TensorRT的关系：\\n\\n集成和优化：Triton Inference Server集成了TensorRT，可以使用TensorRT对模型进行优化，从而在NVIDIA GPU上实现高效的推理。这意味着Triton可以利用TensorRT提供的层与层的融合、精度校准、内核自动调优等优化技术。\\n\\n多模型支持：Triton支持多种深度学习框架，包括TensorFlow、PyTorch、ONNX Runtime等。当使用这些框架训练的模型部署到Triton时，Triton可以利用TensorRT对模型进行进一步的优化，以提高推理性能。\\n\\n易用性和灵活性：Triton提供了一个统一的API和简单的模型管理界面，使得部署和运行经过TensorRT优化的模型变得容易。同时，Triton还支持模型的版本管理和动态加载，提供了高度的灵活性和便利性。\\n\\n跨平台支持：Triton和TensorRT都支持多种操作系统，如Windows、Linux等，这使得它们可以在不同的环境中部署和使用。\\n\\n总的来说，Triton Inference Server和TensorRT是相互补充的工具。Triton提供了模型部署和管理的平台，而TensorRT提供了模型优化的工具。通过将两者结合使用，开发者可以轻松地部署和运行经过优化的深度学习模型，以满足高性能推理的需求。\\n\\nONNX runtime 与 Triton\\n\\nONNX Runtime和Triton Inference Server都是用于深度学习模型推理的开源工具，但它们在设计和功能上有一些区别和联系。\\n\\nONNX Runtime\\n\\n功能：ONNX Runtime是一个开源的推理引擎，用于在各种硬件平台上运行ONNX（Open Neural Network Exchange）格式的深度学习模型。\\n\\n优势：\\n\\n多平台支持：支持多种操作系统和硬件平台，包括CPU、GPU和TPU。\\n\\n简单易用：提供简洁的API和命令行工具，易于集成到应用程序中。\\n\\n模型兼容性：支持多种深度学习框架，如PyTorch、TensorFlow等，便于在不同框架间迁移模型。\\n\\nTriton Inference Server\\n\\n功能：Triton Inference Server是一个高性能的推理引擎，由NVIDIA开发，特别适用于在NVIDIA GPU上运行深度学习模型。\\n\\n优势：\\n\\n优化和加速：通过集成TensorRT，Triton能够对模型进行优化，提高推理速度和降低延迟。\\n\\n多模型管理：支持同时部署和管理多个模型，能够根据请求动态加载和卸载模型。\\n\\n易于部署：提供简单的API和命令行工具，便于部署和运行模型。\\n\\n区别和联系\\n\\n优化方式：ONNX Runtime主要通过编译优化和代码生成来提高性能，而Triton则通过集成TensorRT进行更深入的优化，特别是在NVIDIA GPU上。\\n\\n硬件支持：ONNX Runtime支持多种硬件平台，而Triton则特别针对NVIDIA GPU进行了优化。', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\model deploy and accumulate\\\\model_deploy.md'}), Document(page_content='模型部署过程\\n\\n模型部署入门\\n\\n主要过程\\n\\n为了让模型最终能够部署到某一环境上，开发者们可以使用任意一种深度学习框架来定义网络结构，并通过训练确定网络中的参数。之后，模型的结构和参数会被转换成一种只描述网络结构的中间表示，一些针对网络结构的优化会在中间表示上进行。最后，用面向硬件的高性能编程框架（如 CUDA，OpenCL）编写，能高效执行深度学习网络中算子的推理引擎会把中间表示转换成特定的文件格式，并在对应硬件平台上高效运行模型。\\n\\n详细解释将深度学习模型的中间表示（Intermediate Representation, IR）转化为其他硬件能够识别的过程。这个过程通常涉及到以下几个步骤：\\n\\n1. 中间表示（IR）的选择和转换\\n\\n首先，你需要选择一个中间表示格式，如ONNX、TensorFlow Lite、Core ML等，然后将你的模型（无论是使用TensorFlow、PyTorch、Caffe等框架训练的）转换为这种中间表示。这一步通常可以通过各个框架提供的转换工具来完成。\\n\\n2. 模型优化\\n\\n一旦模型被转换为中间表示，下一步通常是进行模型优化。这包括算子融合、权重剪枝、量化等，以提高模型的性能和减少资源消耗。这些优化可以在中间表示的层面上进行，也可以在特定硬件的目标代码生成之前进行。\\n\\n3. 代码生成和编译\\n\\n优化后的模型需要被转换为目标硬件平台上的可执行代码。这一步通常涉及到以下操作：\\n- 图编译：将优化后的模型图转换为特定硬件上的执行图。这可能涉及到硬件无关的优化，如算子融合和内存布局优化。\\n- 代码生成：根据目标硬件平台（如CPU、GPU、FPGA等），生成高效的执行代码。这可能涉及到使用特定于硬件的编译器或工具链。\\n\\n4. 集成和部署\\n\\n生成的代码需要与目标硬件平台上的运行时环境集成。这可能涉及到与特定硬件相关的库和驱动程序的集成。集成完成后，模型就可以在目标硬件上进行部署和推理了。\\n\\n5. 测试和验证\\n\\n在目标硬件上部署模型后，需要对其进行测试和验证，以确保模型的性能和准确性符合预期。这可能涉及到性能基准测试和准确性评估。\\n\\n6. 性能监控和调优\\n\\n在实际运行环境中，可能需要对模型的性能进行监控，并根据需要进行进一步的调优。这可能涉及到对模型或执行代码的调整。\\n\\n在整个过程中，如果目标硬件是GPU，可能会使用CUDA或OpenCL来生成和优化执行代码。CUDA主要用于NVIDIA GPU，而OpenCL适用于多种类型的GPU和其他并行计算设备。这些工具和API允许开发者编写和优化特定于硬件的代码，以充分利用GPU的并行计算能力。\\n\\n模型部署中，TVM 的作用\\n\\nTVM（Tensor Virtual Machine）在整个深度学习模型部署和推理流程中扮演了关键角色。它主要在以下阶段发挥作用：\\n\\n1. 中间表示（IR）的优化和编译\\n\\n模型转换：首先，深度学习模型被转换为中间表示（如ONNX）。TVM可以接收这些中间表示作为输入。（TVM自己的IR 是 Relay IR）\\n\\n优化：TVM提供了一个自动优化框架，可以对中间表示中的算子进行优化，如算子融合、张量化、布局转换等。这些优化有助于提高模型的性能和减少资源消耗。\\n\\n编译：TVM将优化后的中间表示编译成特定硬件平台上高效运行的代码。它支持多种目标平台，包括CPU、GPU、专用加速器等。\\n\\n2. 高性能编程框架的集成', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\model deploy and accumulate\\\\deploy\\\\deploy_process.md'})]\n"
     ]
    }
   ],
   "source": [
    "print(a['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "022ee1fa-6bda-4e78-9d32-aa0fa7985998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是RAG 融合\n"
     ]
    }
   ],
   "source": [
    "print(a['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08a96707-2f23-4ae2-9cfc-918af612dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @classmethod\n",
      "    def from_documents(\n",
      "        cls: Type[Chroma],\n",
      "        documents: List[Document],\n",
      "        embedding: Optional[Embeddings] = None,\n",
      "        ids: Optional[List[str]] = None,\n",
      "        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n",
      "        persist_directory: Optional[str] = None,\n",
      "        client_settings: Optional[chromadb.config.Settings] = None,\n",
      "        client: Optional[chromadb.Client] = None,  # Add this line\n",
      "        collection_metadata: Optional[Dict] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Chroma:\n",
      "        \"\"\"Create a Chroma vectorstore from a list of documents.\n",
      "\n",
      "        If a persist_directory is specified, the collection will be persisted there.\n",
      "        Otherwise, the data will be ephemeral in-memory.\n",
      "\n",
      "        Args:\n",
      "            collection_name (str): Name of the collection to create.\n",
      "            persist_directory (Optional[str]): Directory to persist the collection.\n",
      "            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      "            documents (List[Document]): List of documents to add to the vectorstore.\n",
      "            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      "            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      "            collection_metadata (Optional[Dict]): Collection configurations.\n",
      "                                                  Defaults to None.\n",
      "\n",
      "        Returns:\n",
      "            Chroma: Chroma vectorstore.\n",
      "        \"\"\"\n",
      "        texts = [doc.page_content for doc in documents]\n",
      "        metadatas = [doc.metadata for doc in documents]\n",
      "        return cls.from_texts(\n",
      "            texts=texts,\n",
      "            embedding=embedding,\n",
      "            metadatas=metadatas,\n",
      "            ids=ids,\n",
      "            collection_name=collection_name,\n",
      "            persist_directory=persist_directory,\n",
      "            client_settings=client_settings,\n",
      "            client=client,\n",
      "            collection_metadata=collection_metadata,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM, BaseLLM\n",
    "from langchain.vectorstores import Chroma\n",
    "import inspect\n",
    "\n",
    "print(inspect.getsource(Chroma.from_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952977fd-e7dc-4ce9-9e45-2a9f818d69da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
