{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f1fb16-6a0e-42f7-8bfb-ef70ed372011",
   "metadata": {},
   "source": [
    "# 本地模型部署(chatGLM 6b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3ce7b-7c5e-454b-8296-934817703f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    源自清华大学 github 网站上的案例（https://github.com/THUDM/ChatGLM-6B）\n",
    "\n",
    "    调试量化 int4级别\n",
    "\n",
    "    先保证本地安装调试完CUDA，然后创建conda 环境 进行环境配置 pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple\n",
    "\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = \"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device='cuda')\n",
    "# 按需修改，目前只支持 4/8 bit 量化\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response + '\\n')\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca471120-9a97-4de4-896a-753e89b83474",
   "metadata": {},
   "source": [
    "# 上面这种模型 定义 是不能够放入到 `RetrievalQA` 中的，因为这里的 `model` 是 `AutoModel` 类型，并不是可以 `Runnable` 的\n",
    "\n",
    "**这样部署的模型直接拿去用是不对的，会出现下面错误：**\n",
    "\n",
    "```python  \n",
    "ValidationError: 2 validation errors for LLMChain\n",
    "\n",
    "\n",
    "llm\n",
    "  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n",
    "llm\n",
    "  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6985e23-a7c6-4800-ab55-cf8bf97662a6",
   "metadata": {},
   "source": [
    "# 正确的 `model` 定义如下：(这个还有点小问题，后面研究)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f3e72f-1aa0-4509-9f6e-6d942f249195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:14:27.001926100Z",
     "start_time": "2024-06-04T01:14:24.777361400Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class HuggingfaceModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = self.model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface\"\n",
    "\n",
    "    def generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> List[str]:\n",
    "        callback_manager = CallbackManagerForLLMRun.get_instance()\n",
    "        with callback_manager.as_context():\n",
    "            callback_manager.on_llm_start({self._llm_type: len(prompts)})\n",
    "            results = [self._call(prompt, stop) for prompt in prompts]\n",
    "            callback_manager.on_llm_end({self._llm_type: len(prompts)})\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3bd31-bd9d-4189-8213-a9b64bdf4ffa",
   "metadata": {},
   "source": [
    "# 下面这个是没问题的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c424c9-a704-42cf-bc82-64a1630c0b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:18:38.010439900Z",
     "start_time": "2024-06-04T01:18:36.599754700Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "class ChatGLM_LLM(LLM):\n",
    "    # 基于本地 InternLM 自定义 LLM 类\n",
    "    tokenizer : AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: InternLM 模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(torch.bfloat16).cuda()\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        # 重写调用函数\n",
    "        response, history = self.model.chat(self.tokenizer, prompt , history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM3-6B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731b06c-7cf1-4f57-8514-7e69d89aa97b",
   "metadata": {},
   "source": [
    "# 本地知识库搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6233126-4a05-4691-b5a1-8ff92b8da4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "# 加载文件夹中的所有.md类型的文件\n",
    "loader = DirectoryLoader(r'D:/Notes/NLP review', glob='**/*.md')\n",
    "# 将数据转成 document 对象，每个文件会作为一个 document\n",
    "documents = loader.load()\n",
    "\n",
    "# 初始化加载器\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "# 切割加载的 document\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 指定 Hugging Face 的预训练模型名称\n",
    "model_name = r\"D:\\CodeLibrary\\ChatGLM\\embedtext2vec\"  # 示例模型名称，您可以根据需要更改\n",
    "\n",
    "# 创建 HuggingFaceEmbeddings 对象\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# 构建持久化的 向量数据库    \n",
    "# vertorstore 持久化地址\n",
    "persist_directory = r'D:\\CodeLibrary\\ChatGLM\\langchain_QA\\chroma_data'\n",
    "\n",
    "#将数据转化为 embeddings 存入chroma,并设置本地磁盘持久化路径\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# 持久化 chroma 0.4x版本开始取消了手动 persist,否则会警告，貌似还无法持久化(改为自动持久化了)\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b244104-b7ef-49ea-a2fa-ea876eaf6987",
   "metadata": {},
   "source": [
    "# langchain集成                 \n",
    "1. 直接导入之前持久化的向量数据库，无需重新构建           \n",
    "2. 定义直接重写LLM接口的llm          \n",
    "3. 通过 langchain 提供的 `RetirevalQA` 对象，构建RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4716c927-9cf6-4cb7-b163-e35adce9571d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:18:51.354487500Z",
     "start_time": "2024-06-04T01:18:47.581936500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1784c0e750614f04a9f08e70766d04c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. 将 document 通过 HuggingFace 的 embeddings 模型计算 embedding 向量信息并临时存入 Chroma 向量数据库，用于后续匹配查询\n",
    "\n",
    "2. 后面在 RetrievalQA.from_chain_type 中, 使用 retriever=docsearch.as_retriever()\n",
    "\n",
    "'''\n",
    "# docsearch = Chroma.from_documents(split_docs, embeddings)\n",
    "\n",
    "\n",
    "# 加载 向量数据库           \n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 初始化自定义 llm \n",
    "model_name = r\"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "\n",
    "chatglm2 = ChatGLM_LLM(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211d00f-c420-44a1-9201-628f9f22cbed",
   "metadata": {},
   "source": [
    "# 检索问答链 参考代码\n",
    "```python\n",
    "store = Chroma.load(\"chroma_store\", embeddings)\n",
    "\n",
    "template = \"\"\"基于以下信息来回答用户问题。                          \n",
    "已知信息：   \n",
    "{context} \n",
    "问题：\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "qa = RetrievalQA.from_chain_type(llm=chatglm, retriever=store.as_retriever(), chain_type=\"stuff\",\n",
    "                                chain_type_kwargs=chain_type_kwargs, return_source_documents=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49d0206-57fd-4625-be01-ff917e7ed3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '什么是RAG？', 'result': 'RAG（Reinforcement-Adversarial Generative）是一种基于博弈理论的生成对抗网络，旨在解决生成式任务中的对抗问题。它由两个部分组成：生成器（Generator）和判别器（Discriminator）。\\n\\n生成器的目标是生成尽可能逼真的样本，以欺骗判别器；判别器的目标是区分真实样本和生成样本，以评估生成器的性能。\\n\\nRAG 通过对损失函数进行优化，使得生成器能够生成更逼真的样本，同时判别器也能够更好地识别真实样本和生成样本。这种方法在生成式任务中取得了很好的效果，如文本生成、图像生成等。', 'source_documents': [Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "template = '''\n",
    "基于以下信息来回答用户问题。如果你不知道答案，就说你不知道，不要试图编造答案。尽量使答案简单，并最后回答的最后说“谢谢你的提问！”。                      \n",
    "已知信息： \n",
    "{context} \n",
    "问题：\n",
    "{question}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "\n",
    "# 创建问答对象\n",
    "# qa = RetrievalQA.from_chain_type(llm=hf_model, chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chatglm2,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 进行问答 Chain\n",
    "\"\"\"\n",
    "    类的 __call__ 方法已经被弃用， 用 invoke 方法来代替 __call__ 方法\n",
    "\"\"\"\n",
    "result = qa.invoke({\"query\": \"什么是RAG？\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b6ea12-4847-4147-a737-1b8940a978ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query', 'result', 'source_documents'])\n"
     ]
    }
   ],
   "source": [
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b86b01-420d-4a06-9227-097d532af44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是RAG？\n"
     ]
    }
   ],
   "source": [
    "print(result['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1937d4b7-0bef-43e2-af5e-86a3523c47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Reinforcement-Adversarial Generative）是一种基于博弈理论的生成对抗网络，旨在解决生成式任务中的对抗问题。它由两个部分组成：生成器（Generator）和判别器（Discriminator）。\n",
      "\n",
      "生成器的目标是生成尽可能逼真的样本，以欺骗判别器；判别器的目标是区分真实样本和生成样本，以评估生成器的性能。\n",
      "\n",
      "RAG 通过对损失函数进行优化，使得生成器能够生成更逼真的样本，同时判别器也能够更好地识别真实样本和生成样本。这种方法在生成式任务中取得了很好的效果，如文本生成、图像生成等。\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752939f6-515e-4052-a49a-ab3d487a4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'}), Document(page_content='ELECTRA and RTD任务\\n\\n参考资料\\n\\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\n\\n完胜 BERT，谷歌优秀 NLP 预训练模型开源\\n\\n核心思想 -RTD\\n\\nELECTRA 使用一种称为替换令牌检测（RTD）的新预训练任务，该任务在从所有输入位置（如：LM）学习的同时，训练双向模型（如：MLM）。\\n\\n具体而言，ELECTRA 的目标是学习区分输入的词。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。\\n\\n然后模型再训练一个判别器，来预测每个词是原始词还是替换词。而判别器的一个优点则是： ==模型从输入的所有词中学习==，而不是像 MLM 那样，仅使用掩盖的词，因此计算更加有效。\\n\\n正如很多开发者联想到的对抗学习方法，ELECTRA 确实受到到生成对抗网络的启发（GAN）。但不同的是，模型采用的是最大似然而非对抗学习。\\n\\n例如下图中，单词「cooked」可以替换为「ate」。尽管这有些道理，但它并不适合整个上下文。预训练任务需要模型（即鉴别器）来确定原始输入中的哪些标记已被替换或保持相同。\\n\\n正是由于该模型的二进制分类任务适用于每个输入单词，而非仅有少量的掩码单词（在 BERT 样式的模型中为 15％），因此，RTD 方法的效率比 MLM 高。这也解释了为什么 ELECTRA 只需更少的示例，就可以达到与其它语言模型相同性能的原因。\\n\\n具体架构\\n\\n替换令牌来自生成器的神经网络。生成器的目标是训练掩码语言模型，即给定输入序列后，按照一定的比例（通常 15%）将输入中的词替换成掩码；然后通过网络得到向量表示；之后再采用 softmax 层，来预测输入序列中掩盖位置的词。\\n\\n尽管生成器的结构类似于 GAN，但由于难以将该方法应用于文本任务，因此得到的训练目标函数为掩盖词的最大似然。\\n\\n之后，生成器和判别器共享相同的输入词嵌入。判别器的目标是判断输入序列每个位置的词是否被生成器替换，如果与原始输入序列对应位置的词不相同，就判别为已替换。\\n\\n补充\\n\\n在Electra模型中，RTD（Replaced Token Detection）任务是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。RTD任务是Electra模型的核心组成部分之一，有助于提高模型对上下文理解和token级别的语义理解能力。\\n\\n下面是RTD任务的详细解释：\\n\\n目标：\\n\\nRTD任务的主要目标是通过模型在输入句子中的某些token上进行替换，并要求模型预测哪些token被替换了来进行模型训练。这使得模型需要更好地理解上下文信息和语义关系，以正确地识别被替换的token。\\n\\n任务流程：\\n\\n给定一个输入句子，首先从该句子中随机选择一些token，并将它们替换为特殊的MASK标记。这些被替换的token将作为模型的输入。(生成器干的事)\\n\\n接下来，模型被要求预测哪些token被替换了。也就是说，模型需要在经过替换的token位置上输出一个二元分类的概率分布，来表示该位置是否被替换了。（判别器干的事）\\n\\n训练目标：\\n\\n在RTD任务中，模型的训练目标是最大化正确预测被替换token的概率，同时最小化其他token的概率。这可以通过最大化交叉熵损失函数来实现。\\n\\n作用：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\base\\\\ELECTRA\\\\ELECTRA_and_RTD.md'})]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9cc4c83-2570-46f5-8d38-664c3131f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @classmethod\n",
      "    def from_chain_type(\n",
      "        cls,\n",
      "        llm: BaseLanguageModel,\n",
      "        chain_type: str = \"stuff\",\n",
      "        chain_type_kwargs: Optional[dict] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> BaseRetrievalQA:\n",
      "        \"\"\"Load chain from chain type.\"\"\"\n",
      "        _chain_type_kwargs = chain_type_kwargs or {}\n",
      "        combine_documents_chain = load_qa_chain(\n",
      "            llm, chain_type=chain_type, **_chain_type_kwargs\n",
      "        )\n",
      "        return cls(combine_documents_chain=combine_documents_chain, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(RetrievalQA.from_chain_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3882d8c-5602-48b7-9d03-b30e05800d80",
   "metadata": {},
   "source": [
    "# 看了下知识库中的内容再进行一次查询发现是有效的（之前效果不好可能是因为知识库的原因，以及部署chatglm6b 我选择的精度比较低）,还可能是因为我使用 CharacterTextSplitter 的时候 overlap设置的为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3b48d08-90eb-4930-9655-ae133a527bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 融合是一种将检索和生成过程进行有效整合的方法，旨在保留检索和生成各自的优势，并更加有效地利用两者之间的关系，从而提高整体系统的性能和效果。它可以通过以下几种方式实现：\n",
      "\n",
      "1. 交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\n",
      "2. 信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\n",
      "3. 多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\n",
      "4. 端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\n",
      "\n",
      "通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\n"
     ]
    }
   ],
   "source": [
    "a = qa.invoke({'query':'什么是RAG 融合'})\n",
    "print(a['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21594fa3-9b64-47ab-bdbb-07c7eb22d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'}), Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'}), Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'}), Document(page_content='模型管理：Triton提供了模型版本管理和模型仓库功能，使得模型部署和更新更加灵活和方便。\\n\\n多模型服务：Triton支持同时部署多个模型，并能够根据请求动态加载和卸载模型，这对于需要处理多种任务的场景非常有用。\\n\\n自定义后端：Triton允许开发者编写自定义的后端插件，以支持新的模型格式或实现特定的推理逻辑。\\n\\n与PyTorch的关系\\n\\n模型转换：要将PyTorch模型部署到Triton上，通常需要先将模型转换为ONNX格式。ONNX是一个开放的格式，用于表示深度学习模型，它被Triton等推理引擎广泛支持。\\n\\n推理加速：Triton可以利用TensorRT等工具对ONNX模型进行优化，从而在NVIDIA GPU上实现高效的推理。\\n\\n部署和管理：一旦模型转换为ONNX格式，就可以使用Triton来部署和管理这些模型，提供高效的推理服务。\\n\\n总之，Triton是一个强大的推理引擎，它可以与PyTorch等深度学习框架协同工作，帮助开发者高效地部署和运行深度学习模型。\\n\\nTensorRT\\n\\nTensorRT是NVIDIA推出的一款深度学习推理（Inference）引擎，它专门用于优化深度学习模型以实现高性能的推理。TensorRT通过将深度学习模型（如CNN、RNN等）转换为优化的推理引擎，能够在NVIDIA GPU上提供高效的推理性能。\\n\\nTensorRT的主要特点包括：\\n\\n模型优化：TensorRT可以对深度学习模型进行层与层的融合、精度校准、内核自动调优等优化，以提高推理速度和减少内存占用。\\n\\n支持多种框架：TensorRT支持多种深度学习框架，如Caffe、TensorFlow、PyTorch等。用户可以将这些框架训练的模型转换为TensorRT可以理解的格式。\\n\\n多种精度支持：TensorRT支持FP32、FP16、INT8等多种精度，允许用户根据性能和精度需求进行选择。\\n\\n易于集成：TensorRT提供了C++和Python API，可以轻松集成到现有的应用程序中。\\n\\n跨平台支持：TensorRT可以在多种操作系统上运行，包括Windows、Linux等。\\n\\n使用TensorRT的步骤通常包括：\\n\\n模型转换：首先，需要将训练好的模型从原始的深度学习框架（如PyTorch、TensorFlow等）转换为TensorRT可以理解的格式，如ONNX。\\n\\n优化和校准：使用TensorRT对模型进行优化，包括层与层的融合、精度校准等。\\n\\n构建推理引擎：将优化后的模型构建为TensorRT推理引擎。\\n\\n推理：使用构建好的推理引擎进行推理，处理输入数据并生成输出。\\n\\nTensorRT特别适用于需要高性能推理的场景，如自动驾驶、视频分析、语音识别等。通过使用TensorRT，开发者可以在不牺牲太多精度的情况下显著提高模型的推理速度，从而满足实时性要求。\\n\\nTriton 与 TensorRT\\n\\nTriton Inference Server和TensorRT之间有密切的关系。Triton Inference Server是一个开源的推理引擎，由NVIDIA开发，旨在提供高性能、灵活的深度学习模型推理服务。而TensorRT是NVIDIA的一个深度学习推理优化工具，用于优化和加速深度学习模型的推理性能。\\n\\nTriton与TensorRT的关系：', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\model deploy and accumulate\\\\model_deploy.md'})]\n"
     ]
    }
   ],
   "source": [
    "print(a['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "022ee1fa-6bda-4e78-9d32-aa0fa7985998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是RAG 融合\n"
     ]
    }
   ],
   "source": [
    "print(a['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a96707-2f23-4ae2-9cfc-918af612dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @classmethod\n",
      "    def from_documents(\n",
      "        cls: Type[Chroma],\n",
      "        documents: List[Document],\n",
      "        embedding: Optional[Embeddings] = None,\n",
      "        ids: Optional[List[str]] = None,\n",
      "        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n",
      "        persist_directory: Optional[str] = None,\n",
      "        client_settings: Optional[chromadb.config.Settings] = None,\n",
      "        client: Optional[chromadb.Client] = None,  # Add this line\n",
      "        collection_metadata: Optional[Dict] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Chroma:\n",
      "        \"\"\"Create a Chroma vectorstore from a list of documents.\n",
      "\n",
      "        If a persist_directory is specified, the collection will be persisted there.\n",
      "        Otherwise, the data will be ephemeral in-memory.\n",
      "\n",
      "        Args:\n",
      "            collection_name (str): Name of the collection to create.\n",
      "            persist_directory (Optional[str]): Directory to persist the collection.\n",
      "            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      "            documents (List[Document]): List of documents to add to the vectorstore.\n",
      "            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      "            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      "            collection_metadata (Optional[Dict]): Collection configurations.\n",
      "                                                  Defaults to None.\n",
      "\n",
      "        Returns:\n",
      "            Chroma: Chroma vectorstore.\n",
      "        \"\"\"\n",
      "        texts = [doc.page_content for doc in documents]\n",
      "        metadatas = [doc.metadata for doc in documents]\n",
      "        return cls.from_texts(\n",
      "            texts=texts,\n",
      "            embedding=embedding,\n",
      "            metadatas=metadatas,\n",
      "            ids=ids,\n",
      "            collection_name=collection_name,\n",
      "            persist_directory=persist_directory,\n",
      "            client_settings=client_settings,\n",
      "            client=client,\n",
      "            collection_metadata=collection_metadata,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM, BaseLLM\n",
    "from langchain.vectorstores import Chroma\n",
    "import inspect\n",
    "\n",
    "print(inspect.getsource(Chroma.from_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163357a8-d2ca-42a4-b4ac-9311d7bfba99",
   "metadata": {},
   "source": [
    "# 仅用大模型回答看看答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5b8252-b8dd-417c-9bc0-2e3f727b3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_only_result = chatglm2(\"什么是RAG\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a11d99-d484-4a51-89cd-f4791f88db35",
   "metadata": {},
   "source": [
    "# 可以明显看到，没有本地知识库的帮助，答案是非常糟糕的（虽然有本地知识库也有错误。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e280cb24-4ec9-423a-a51d-9ec9942355ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG是指残骸（R骸），通常是指在战争、自然灾害或其他暴力事件中失去生命的动物或人类遗体。这些遗体可能被遗弃在公共场所，例如街道、广场或公园等。 RAG对于战时医疗工作者来说，是一个重要的词汇，意味着需要立即处理和妥善处理这些遗体，以防止疾病传播和环境污染。\n"
     ]
    }
   ],
   "source": [
    "print(llm_only_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee4ae24-7c9b-4b41-aad8-f8398ed8ca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG融合（RAG-Fusion）是一种将来自不同RAG（Read-Ahead Generation）技术的数据进行融合的技术，旨在提高数据处理的效率和准确性。通过将来自多个RAG的数据进行融合，可以避免在处理过程中产生重复的信息，从而提高数据的一致性和完整性。\n",
      "\n",
      "RAG融合技术可以在多种应用场景中使用，如在分布式文件系统、大数据处理系统、数据库系统中等。常见的RAG包括：\n",
      "\n",
      "1. 基于哈希的RAG（如Hadoop Distributed File System中的RAG）\n",
      "2. 基于内容的RAG（如HBase中的RAG）\n",
      "3. 基于时间的RAG（如TimeSeriesDB中的RAG）\n",
      "\n",
      "RAG融合方法可以有多种，如简单的拼接、基于统计的融合、基于机器学习的融合等。在实际应用中，需要根据具体场景和需求选择合适的RAG融合方法。\n"
     ]
    }
   ],
   "source": [
    "llm_only_result2 = chatglm2(\"什么是RAG融合？\")\n",
    "print(llm_only_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e430a-d5f1-438f-80d6-f4f3607ac41a",
   "metadata": {},
   "source": [
    "# 部署 Web Demo   \n",
    "根据之前的到的结果：\n",
    "```python\n",
    "result = qa.invoke({\"query\": \"什么是RAG？\"})\n",
    "print(result)  \n",
    "```\n",
    "可以将这些内容进行封装，通过gradio 进行web部署，主要流程如下:\n",
    "1. 将之前的langchain问答链相关部分进行封装变成一个对象  \n",
    "2. 启动gradio，通过该对象进行知识问答      \n",
    "\n",
    "## 先进行封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fcfc3-59a9-4a74-b8f8-11070d002525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from LLM import ChatGLM_LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def load_chain():\n",
    "    # 加载问答链\n",
    "    # 定义 Embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=r\"D:\\CodeLibrary\\ChatGLM\\embedtext2vec\")\n",
    "\n",
    "    # 向量数据库持久化路径\n",
    "    persist_directory = r'D:\\CodeLibrary\\ChatGLM\\langchain_QA\\chroma_data'\n",
    "\n",
    "    # 加载数据库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,  # 允许我们将persist_directory目录保存到磁盘上\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "    # 加载自定义 LLM\n",
    "    chatglm = ChatGLM_LLM(model_path = r\"D:\\CodeLibrary\\ChatGLM\\chatglm26b\")\n",
    "\n",
    "    # 定义一个 Prompt Template\n",
    "    template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "    案。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。 \n",
    "    {context} \n",
    "    问题: {question} \n",
    "    有用的回答:\"\"\" \n",
    "\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],template=template)\n",
    "\n",
    "\n",
    "    # 运行 chain \n",
    "    qa_chain = RetrievalQA.from_chain_type(chatlm,retriever=vectordb.as_retriever(),return_source_documents=True,chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "# 该类负责加载并存储检索问答链，并响应 Web 界面里调用检索问答链进行回答的动作\n",
    "class Model_center():\n",
    "    \"\"\"\n",
    "    存储检索问答链的对象 \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 构造函数，加载检索问答链\n",
    "        self.chain = load_chain()\n",
    "\n",
    "    def qa_chain_self_answer(self, question: str, chat_history: list = []):\n",
    "        \"\"\"\n",
    "        调用问答链进行回答\n",
    "        \"\"\"\n",
    "        if question == None or len(question) < 1:\n",
    "            return \"\", chat_history\n",
    "        try:\n",
    "            chat_history.append(\n",
    "                (question, self.chain({\"query\": question})[\"result\"]))\n",
    "            # 将问答结果直接附加到问答历史中，Gradio 会将其展示出来\n",
    "            return \"\", chat_history\n",
    "        except Exception as e:\n",
    "            return e, chat_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45357a80-020e-4a97-9e42-ce520a4e5181",
   "metadata": {},
   "source": [
    "## gradio 部分  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6f431-8a86-49ec-b4f5-8d88876c993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# 实例化核心功能对象\n",
    "model_center = Model_center()\n",
    "# 创建一个 Web 界面\n",
    "block = gr.Blocks()\n",
    "with block as demo:\n",
    "    with gr.Row(equal_height=True):   \n",
    "        with gr.Column(scale=15):\n",
    "            # 展示的页面标题\n",
    "            gr.Markdown(\"\"\"<h1><center>基于ChatGLM2的本地知识问答</center></h1>\n",
    "                <center>base ChatGLM2-6B + chroma</center>              \n",
    "                \"\"\")  \n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            # 创建一个聊天机器人对象\n",
    "            chatbot = gr.Chatbot(height=450, show_copy_button=True)\n",
    "            # 创建一个文本框组件，用于输入 prompt。\n",
    "            msg = gr.Textbox(label=\"Prompt/问题\")\n",
    "\n",
    "            with gr.Row():\n",
    "                # 创建提交按钮。\n",
    "                db_wo_his_btn = gr.Button(\"Chat\")\n",
    "            with gr.Row():\n",
    "                # 创建一个清除按钮，用于清除聊天机器人组件的内容。\n",
    "                clear = gr.ClearButton(\n",
    "                    components=[chatbot], value=\"Clear console\")\n",
    "                \n",
    "        # 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "        db_wo_his_btn.click(model_center.qa_chain_self_answer, inputs=[\n",
    "                            msg, chatbot], outputs=[msg, chatbot])\n",
    "        \n",
    "    gr.Markdown(\"\"\"提醒：<br>\n",
    "    1. 初始化数据库时间可能较长，请耐心等待。\n",
    "    2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 <br>\n",
    "    \"\"\")\n",
    "gr.close_all()\n",
    "# 直接启动\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
