{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f762543-ff32-4ebe-952d-53431749fb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21b9877bbc7405ab5b442edd0a6428f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\chatGLM_env\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n",
      "\n",
      "睡眠对身体健康和心理健康都很重要，如果你晚上睡不着，可以尝试以下一些方法来帮助自己入睡:\n",
      "\n",
      "1. 改善睡眠环境:确保睡眠环境安静、舒适、黑暗、凉爽。如果睡眠环境不合适，会感到疲劳和不安，从而更难入睡。\n",
      "\n",
      "2. 放松身心:在睡觉前一小时，放松身心是很重要的。你可以做些轻松的伸展运动、冥想或进行伸展运动，或做瑜伽。\n",
      "\n",
      "3. 规律作息:尽量在同一时间入睡和起床，即使在周末也应该保持相同的作息时间，这有助于调节身体的生物钟。\n",
      "\n",
      "4. 避免刺激:睡觉前一小时，尽量避免看电视、玩游戏或进行刺激性的活动，因为这可能会让你兴奋和不安。\n",
      "\n",
      "5. 避免咖啡因和酒精:咖啡因和酒精都可能影响睡眠质量，因此在睡觉前一小时，尽量避免摄入这些物质。\n",
      "\n",
      "6. 远离压力:在睡觉前一小时，尽量避免进行紧张的活动，如激烈的运动或紧张的工作。\n",
      "\n",
      "如果这些方法不能解决你的问题，可以尝试寻求医生的帮助，找到更好的解决方案。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    源自清华大学 github 网站上的案例（https://github.com/THUDM/ChatGLM-6B）\n",
    "\n",
    "    调试量化 int4级别\n",
    "\n",
    "    先保证本地安装调试完CUDA，然后创建conda 环境 进行环境配置 pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = \"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device='cuda')\n",
    "# 按需修改，目前只支持 4/8 bit 量化\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response + '\\n')\n",
    "\n",
    "response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "print(response + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfbe3d-3413-497f-91e2-e22fe2cdedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"还有其他的吗\", history=history)\n",
    "print(response + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8c1fb-45b4-49f1-ad80-1c5e9378f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "print(\"当前工作目录:\", current_working_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4fb98d-9204-407d-aeff-080aea2230cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd01587-30bc-489a-9de9-61aa0fa81e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看有哪些类\n",
    "from langchain import embeddings\n",
    "import inspect\n",
    "classes = [m[0] for m in inspect.getmembers(embeddings, inspect.isclass)]\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e87fd-f28c-4317-9387-bfc7931517a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看包里面具体的类 代码\n",
    "print(inspect.getsource(embeddings.HuggingFaceHubEmbeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ee91a-c1ca-4ddf-9277-831d35fe3695",
   "metadata": {},
   "source": [
    "# 根据langchain 和 Faiss构建知识向量库       \n",
    "\n",
    "其中使用 `pip install faiss-gpu` 安装 GPU版本的 Faiss向量数据库\n",
    "\n",
    "\n",
    "\n",
    "首先引入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520324b-ef16-44ef-b181-82f01af9fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS    \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ae747-093f-4295-8b9a-718ea61a9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb32914-8488-47d5-b87a-15e2a943d2c3",
   "metadata": {},
   "source": [
    "# 取消用 Faiss，window不支持，或者说要求特别麻烦，并且没讲讲清楚固定的依赖，导致无法安装\n",
    "\n",
    "\n",
    "\n",
    "主要体现在cuda版本，还有支不支持gpu的问题；要在linux上比较容易，故放弃（浪费时间配环境）   \n",
    "\n",
    "# 换成milvus～    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc4b8d-652f-4afe-9306-3e263dbda1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# import milvus\n",
    "# from langchain.vectorstores import milvus\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c6339-37c3-4c93-9875-9d20aa7d0d38",
   "metadata": {},
   "source": [
    "# 加载数据，格式为md文件  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e890981c-335a-4ab1-a5d3-e159e1793661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data example\n",
      "`Beam search` 是一种启发式搜索算法，常用于自然语言处理中的解码阶段，特别是在机器翻译、语音识别和文本生成等任务中。它的主要目的是在解码过程中保持一个候选列表（即“beam”），这个列表中的每个元素都是一个部分构建的序列（如句子的一部分），并且根据某种评分机制（如概率）对这些候选进行排序和筛选。\n",
      "\n",
      "`Beam search` 的**核心思想**是在每个时间步（或每个词汇的生成步骤）不仅仅考虑单一的最优候选，而是维护一个大小为 B 的候选集（B 通常被称为 beam width 或 beam size），这样可以在解码过程中探索多条路径。在每一步，算法都会根据当前的候选集扩展出新的候选集，并从中选择评分最高的 B 个候选，作为下一步的候选集。这个过程一直持续到生成序列的结束符被添加到某个候选中，或者达到序列的最大长度限制。\n",
      "\n",
      "\n",
      "下面通过一个简单的例子来说明 beam search 的过程：\n",
      "假设我们有一个语言模型，它在每个时间步都会为下一个词汇生成一个概率分布。我们要使用这个模型来生成一个句子，并且设置 beam size 为 3。\n",
      "1. 第一步，模型生成了三个词汇的概率分布，我们选择概率最高的三个词汇作为当前步的候选：[\"我\"，\"你\"，\"他\"]。\n",
      "2. 第二步，对于每个候选，模型生成接下来一个词汇的概率分布。我们为每个候选词汇组合计算一个联合概率，并选择概率最高的三个组合作为新的候选集。例如，可能的新候选集是：[\"我 爱\"，\"你 是\"，\"他 的\"]。\n",
      "3. 重复这个过程，直到某个候选序列以句子结束符（如句号）结束，或者达到序列的最大长度。\n",
      "\n",
      "\n",
      "`Beam search` 允许算法在解码过程中考虑多个可能的候选，从而**在一定程度上平衡了贪心算法（每次选择最优候选）和穷举搜索（考虑所有可能的候选）之间的权衡**。通过调整 beam size，我们可以控制算法在解码过程中探索的广度和深度。较大的 beam size 可以提高找到最优解的可能性，但同时也会增加计算成本。\n",
      "需要注意的是，Beam search 并不保证能找到最优解，因为它可能会在解码过程中丢弃最终的最优路径。此外，Beam search 也不保证能得到多样性很强的输出，因为它倾向于选择概率较高的候选，这可能会导致生成的句子比较单一。为了解决这个问题，有时会采用一些额外的技术，如随机化或长度惩罚，来增加输出句子的多样性。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ps = list(Path('D:/Notes/NLP review').glob(\"**/*.md\"))\n",
    "\n",
    "data = []\n",
    "sources = []\n",
    "for p in ps:\n",
    "    with open(p, encoding='utf8') as f:\n",
    "        data.append(f.read())\n",
    "    sources.append(p)\n",
    "print(\"data example\")\n",
    "print(data[0])\n",
    "# print(\"-\"*50)\n",
    "# print(sources[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3472e-6532-4912-a366-eb2c96dea78b",
   "metadata": {},
   "source": [
    "# 由于 LLM有context限制，故需要spliter，`chunk_size`先随意设置   \n",
    "\n",
    "- `metadatas` 的作用：**每个分割后的文本块仍然需要保留与原始文本的关联信息**  \n",
    "- `splits` 是将 text 分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21eff2f5-7dea-4c8e-819c-61ce288498df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Beam search` 是一种启发式搜索算法，常用于自然语言处理中的解码阶段，特别是在机器翻译、语音识别和文本生成等任务中。它的主要目的是在解码过程中保持一个候选列表（即“beam”），这个列表中的每个元素都是一个部分构建的序列（如句子的一部分），并且根据某种评分机制（如概率）对这些候选进行排序和筛选。\n",
      "`Beam search` 的**核心思想**是在每个时间步（或每个词汇的生成步骤）不仅仅考虑单一的最优候选，而是维护一个大小为 B 的候选集（B 通常被称为 beam width 或 beam size），这样可以在解码过程中探索多条路径。在每一步，算法都会根据当前的候选集扩展出新的候选集，并从中选择评分最高的 B 个候选，作为下一步的候选集。这个过程一直持续到生成序列的结束符被添加到某个候选中，或者达到序列的最大长度限制。\n",
      "下面通过一个简单的例子来说明 beam search 的过程：\n",
      "假设我们有一个语言模型，它在每个时间步都会为下一个词汇生成一个概率分布。我们要使用这个模型来生成一个句子，并且设置 beam size 为 3。\n",
      "1. 第一步，模型生成了三个词汇的概率分布，我们选择概率最高的三个词汇作为当前步的候选：[\"我\"，\"你\"，\"他\"]。\n",
      "2. 第二步，对于每个候选，模型生成接下来一个词汇的概率分布。我们为每个候选词汇组合计算一个联合概率，并选择概率最高的三个组合作为新的候选集。例如，可能的新候选集是：[\"我 爱\"，\"你 是\"，\"他 的\"]。\n",
      "3. 重复这个过程，直到某个候选序列以句子结束符（如句号）结束，或者达到序列的最大长度。\n",
      "`Beam search` 允许算法在解码过程中考虑多个可能的候选，从而**在一定程度上平衡了贪心算法（每次选择最优候选）和穷举搜索（考虑所有可能的候选）之间的权衡**。通过调整 beam size，我们可以控制算法在解码过程中探索的广度和深度。较大的 beam size 可以提高找到最优解的可能性，但同时也会增加计算成本。\n",
      "需要注意的是，Beam search 并不保证能找到最优解，因为它可能会在解码过程中丢弃最终的最优路径。此外，Beam search 也不保证能得到多样性很强的输出，因为它倾向于选择概率较高的候选，这可能会导致生成的句子比较单一。为了解决这个问题，有时会采用一些额外的技术，如随机化或长度惩罚，来增加输出句子的多样性。 docs[0] lens: 1007\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500, separator='\\n')\n",
    "docs = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "for i,d in enumerate(data):\n",
    "    splits = text_splitter.split_text(d)\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in splits]  # 生成唯一的 ID\n",
    "    ids.extend(doc_ids)\n",
    "    docs.extend(splits)\n",
    "    metadatas.extend([{\"source\": str(sources[i])}] * len(splits))\n",
    "print(docs[0], 'docs[0] lens:', len(docs[0]))\n",
    "# print(\"-\"*100)\n",
    "# print(metadatas[0])\n",
    "# print(\"-\"*100)\n",
    "# print(ids[0])\n",
    "# print(type(metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143a04d-e63e-4585-946a-9607732bb8a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 使用 faiss 把文本转换为 vector, 选择huggingface开源的模型 text2vec；通过faiss构建存储到磁盘的知识向量库\n",
    "代码如下：（**因为无法安装faiss导致无法使用**）       \n",
    "```python           \n",
    "store = FAISS.from_texts(docs, HuggingFaceEmbeddings(model_name=\"shibing624/text2vec-base-multilingual\"), metadatas=metadatas)\n",
    "faiss.write_index(store.index, \"docs.index\")               \n",
    "store.index = None\n",
    "with open(\"faiss_store.pkl\", 'wb') as f:\n",
    "    pickle.dump(store, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e7205-552e-4247-9802-4f0ac2ac75d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 发现 milvus不好用，操作太复杂了，使用 chromeadb 平替\n",
    "```python\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# 忽略警告(如果报错或者那里有有问题，导致查不出原因就删掉试试)   \n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# 初始化 ChromaDB 客户端\n",
    "client = chromadb.Client()\n",
    "\n",
    "# 创建一个集合用于存储文档及其嵌入\n",
    "try:\n",
    "    client.delete_collection(name=\"my_collection\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting collection: {e}\")\n",
    "\n",
    "collection = client.create_collection(name=\"my_collection\")\n",
    "\n",
    "# 定义嵌入函数\n",
    "model_name = r\"D:\\CodeLibrary\\ChatGLM\\text2vecmul\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embeddings = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# def embed_texts(texts):\n",
    "#     inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "#     return embeddings.tolist()\n",
    "\n",
    "# 获取嵌入向量 \n",
    "\n",
    "# docs = [\"document1\", \"document2\", \"document3\"]      \n",
    "# metadatas = [{\"meta\": \"data1\"}, {\"meta\": \"data2\"}, {\"meta\": \"data3\"}]\n",
    "# embeddings = embed_texts(docs)\n",
    "\n",
    "# 向集合中添加数据\n",
    "collection.add(ids=ids, documents=docs, metadatas=metadatas, embeddings=embeddings)\n",
    "\n",
    "\"\"\"\n",
    "    (因为无法通过pickle序列化 client中的某些链接, 故用dict方式进行数据序列化，但是没用，因为后续其实需要整个db的内容序列化后的加载)\n",
    "\"\"\"\n",
    "# 提取集合中的数据\n",
    "collection_data = {                   \n",
    "    \"ids\": docs,\n",
    "    \"documents\": docs,\n",
    "    \"metadatas\": metadatas,\n",
    "    \"embeddings\": embeddings,\n",
    "}\n",
    "\n",
    "# 将集合数据保存到文件\n",
    "with open(\"chroma_collection_data.pkl\", 'wb') as f:\n",
    "    pickle.dump(collection_data, f)\n",
    "# 关闭客户端（chromadb 没有 close这种操作）   \n",
    "# client.close()        \n",
    "\n",
    "print(\"done!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a37364-ba90-4aa3-9444-f9ff315a8ded",
   "metadata": {},
   "source": [
    "# 上面的代码又因为是直接用 chromadb，在序列化索引这一块又有问题，不知道怎么解决   \n",
    "\n",
    "# 故，根据 langchain文档，改用 langchain中 集成的 chroma\n",
    "- 前提仍然是要安装 `chromadb`\n",
    "  ```python\n",
    "  pip install chromadb\n",
    "  ```\n",
    "- **不再使用** `client = chromadb.Client()` 初始化客户端，然后再创建 `collection`\n",
    "- **改成了** 通过导包 `from langchain.vectorstores import Chroma`, 然后通过 `db = Chroma.from_documents（doc, embeddings）` 来获取\n",
    "\n",
    "# 更新后的代码如下:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36b9e6-5f29-4694-b303-02377fe97e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# embedding_model 和 tokenizer获取方式不变\n",
    "embed_model_name = r\"D:\\CodeLibrary\\ChatGLM\\text2vecmul\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "\n",
    "def embed_texts(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = embed_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    return embeddings.tolist()\n",
    "\n",
    "# embeddings = embed_texts(docs)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)\n",
    "\n",
    "# 将文本和元数据封装成Document对象  \n",
    "documents = [Document(page_content=text, metadata=meta) for text, meta in zip(docs, metadatas)]\n",
    "print(type(metadatas))\n",
    "store = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "store.persist(\"chroma_store\")  # 持久化存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0e9d0-d663-48b5-9076-e75aea810b6e",
   "metadata": {},
   "source": [
    "# 利用 `inspect` 查看源码    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe30d6c-3c9c-4bd9-8824-df06eebe2009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class RetrievalQA(BaseRetrievalQA):\n",
      "    \"\"\"Chain for question-answering against an index.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.llms import OpenAI\n",
      "            from langchain.chains import RetrievalQA\n",
      "            from langchain.faiss import FAISS\n",
      "            from langchain.vectorstores.base import VectorStoreRetriever\n",
      "            retriever = VectorStoreRetriever(vectorstore=FAISS(...))\n",
      "            retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    retriever: BaseRetriever = Field(exclude=True)\n",
      "\n",
      "    def _get_docs(\n",
      "        self,\n",
      "        question: str,\n",
      "        *,\n",
      "        run_manager: CallbackManagerForChainRun,\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Get docs.\"\"\"\n",
      "        return self.retriever.get_relevant_documents(\n",
      "            question, callbacks=run_manager.get_child()\n",
      "        )\n",
      "\n",
      "    async def _aget_docs(\n",
      "        self,\n",
      "        question: str,\n",
      "        *,\n",
      "        run_manager: AsyncCallbackManagerForChainRun,\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Get docs.\"\"\"\n",
      "        return await self.retriever.aget_relevant_documents(\n",
      "            question, callbacks=run_manager.get_child()\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def _chain_type(self) -> str:\n",
      "        \"\"\"Return the chain type.\"\"\"\n",
      "        return \"retrieval_qa\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import chromadb\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 获取源码\n",
    "source_code = inspect.getsource(RetrievalQA)\n",
    "\n",
    "print(source_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac60cd6-9bb5-4228-b701-1738e3fd9eb1",
   "metadata": {},
   "source": [
    "# 接下来是 访问知识库与用户交互                  \n",
    "\n",
    "这里使用 langchain API `RetrievalQA` 来访问知识库        \n",
    "\n",
    "1. 从磁盘加载向量数据库   \n",
    "```python   \n",
    "index = faiss.read_index('./docs.index') # 之前生成的数据库index\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./faiss_store.pkl\", 'rb') as f:   \n",
    "    store = pickle.load(f)   \n",
    "\n",
    "store.index = index   \n",
    "```\n",
    "\n",
    "2. 设计一个prompt 模板   \n",
    "```python   \n",
    "template = \"\"\"基于以下信息来回答用户问题。                            \n",
    "已知信息：   \n",
    "{context}   \n",
    "问题：\n",
    "{question}\"\"\"    \n",
    "```\n",
    "\n",
    "3. 使用 `RetrievalQA` 来 基于对知识库的访问结果 与 LLM 进行交互    \n",
    "```python                                           \n",
    "prompt = PromplateTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}    \n",
    "qa = RetrievalQA.from_chain_type(llm=chatglm, retriever=store.as_retriever(), chain_type=\"stuff\",\n",
    "                                chain_type_kwargs=chain_type_kwargs, return_source_documents=True)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "4. 构建一个查询输入框，从LLM返回结果中，提取问题答案和知识库文档引用来源：   \n",
    "```python   \n",
    "query = input(\"\\n请输入问题：\")                         \n",
    "\n",
    "res = qa(query)   \n",
    "answer, doc = res['result'], res['source_documents']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n> 问题：')     \n",
    "print(query)    \n",
    "print(\"\\n> 回答：\")         \n",
    "print(answer)     \n",
    "print(\"\\n> 来源：\")                        \n",
    "for document in docs:  \n",
    "    print(\"\\n> \" + str(document.metadata['source']) + \":\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ca067-29d7-4a63-ad69-514f7997f2f6",
   "metadata": {},
   "source": [
    "# chroma 版本的 知识库访问 与 用户交互\n",
    "1. chroma 索引加载\n",
    "2. prompt template 定义  \n",
    "3. 用户交互定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84038e2-8b67-461b-ae33-30a17bdf3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Chroma.load(\"chroma_store\", embeddings)\n",
    "\n",
    "template = \"\"\"基于以下信息来回答用户问题。                          \n",
    "已知信息： \n",
    "{context} \n",
    "问题：\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "qa = RetrievalQA.from_chain_type(llm=chatglm, retriever=store.as_retriever(), chain_type=\"stuff\",\n",
    "                                chain_type_kwargs=chain_type_kwargs, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890d5fe-5156-4355-8e55-77566884d79d",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123fc0d-c134-4f05-9770-9e04d62f2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"\\n请输入问题：\")                       \n",
    "\n",
    "res = qa(query) \n",
    "answer, doc = res['result'], res['source_documents']\n",
    "\n",
    "print('\\n\\n> 问题：')   \n",
    "print(query)  \n",
    "print(\"\\n> 回答：\")       \n",
    "print(answer)   \n",
    "print(\"\\n> 来源：\")                      \n",
    "for document in docs:\n",
    "    print(\"\\n> \" + str(document.metadata['source']) + \":\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
