{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e125911e-3633-43db-910a-10be753202e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5abb0-c505-4bd0-b1f3-7ed2cfd8a928",
   "metadata": {},
   "source": [
    "# 完全不能跑，以后熟悉了再回来填坑 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf6a8eb-dc0d-4107-9fe0-0b2d3757e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class ChatGLMTokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m没有GPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:724\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    722\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 724\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    725\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         )\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Tokenizer class ChatGLMTokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from transformers import AutoModel, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "model_path = \"chatglm2-6b_model\"\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "else:\n",
    "    print('没有GPU')\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a93f6e6-69ac-47d7-aded-9601c2c5da82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading this model requires you to execute execute some code in that repo on your local machine. Make sure you have read the code at https://hf.co/chatglm2-6b_model to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mE:\\software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\dynamic_module_utils.py:593\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 593\u001b[0m     signal\u001b[38;5;241m.\u001b[39msignal(\u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSIGALRM\u001b[49m, _raise_timeout_error)\n\u001b[0;32m    594\u001b[0m     signal\u001b[38;5;241m.\u001b[39malarm(TIME_OUT_REMOTE_CODE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'signal' has no attribute 'SIGALRM'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).half().cuda()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device='cuda')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.15\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mE:\\software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:482\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 482\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mE:\\software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1010\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1008\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1009\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[1;32m-> 1010\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1015\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mE:\\software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\dynamic_module_utils.py:608\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[0;32m    605\u001b[0m         signal\u001b[38;5;241m.\u001b[39malarm(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;66;03m# OS which does not support signal.SIGALRM\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    609\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading this model requires you to execute execute some code in that repo on your local machine. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure you have read the code at https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to avoid malicious use, then set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    612\u001b[0m         )\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_remote_code:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;66;03m# For the CI which puts the timeout at 0\u001b[39;00m\n\u001b[0;32m    615\u001b[0m     _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Loading this model requires you to execute execute some code in that repo on your local machine. Make sure you have read the code at https://hf.co/chatglm2-6b_model to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\n",
    "\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).half().cuda()\n",
    "\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device='cuda')\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    top_p=1,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "glm_model = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187fb4ea-8836-487d-82d9-eb59b33140e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m#context# \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mYou are a good helpful, respectful and honest assistant.You are ready for answering human\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms question and always answer as helpfully as possible, while being safe.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mHuman:What is a good name for a company that makes \u001b[39m\u001b[38;5;132;01m{product}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m      9\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m     template\u001b[38;5;241m=\u001b[39mtemplate\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39m\u001b[43mglm_model\u001b[49m, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m     13\u001b[0m chain\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning shoes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glm_model' is not defined"
     ]
    }
   ],
   "source": [
    "template = '''\n",
    "#context# \n",
    "You are a good helpful, respectful and honest assistant.You are ready for answering human's question and always answer as helpfully as possible, while being safe.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \n",
    "#question# \n",
    "Human:What is a good name for a company that makes {product}?\"\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template\n",
    ")\n",
    "chain = LLMChain(llm=glm_model, prompt=prompt)\n",
    "chain.run(\"running shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9326a-5e30-4e69-a053-ea34bf956a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
