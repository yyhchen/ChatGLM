{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f1fb16-6a0e-42f7-8bfb-ef70ed372011",
   "metadata": {},
   "source": [
    "# 本地模型部署(chatGLM 6b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3ce7b-7c5e-454b-8296-934817703f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    源自清华大学 github 网站上的案例（https://github.com/THUDM/ChatGLM-6B）\n",
    "\n",
    "    调试量化 int4级别\n",
    "\n",
    "    先保证本地安装调试完CUDA，然后创建conda 环境 进行环境配置 pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple\n",
    "\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = \"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device='cuda')\n",
    "# 按需修改，目前只支持 4/8 bit 量化\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response + '\\n')\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca471120-9a97-4de4-896a-753e89b83474",
   "metadata": {},
   "source": [
    "# 上面这种模型 定义 是不能够放入到 `RetrievalQA` 中的，因为这里的 `model` 是 `AutoModel` 类型，并不是可以 `Runnable` 的\n",
    "\n",
    "**这样部署的模型直接拿去用是不对的，会出现下面错误：**\n",
    "\n",
    "```python  \n",
    "ValidationError: 2 validation errors for LLMChain\n",
    "\n",
    "\n",
    "llm\n",
    "  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n",
    "llm\n",
    "  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6985e23-a7c6-4800-ab55-cf8bf97662a6",
   "metadata": {},
   "source": [
    "# 正确的 `model` 定义如下：(这个还有点小问题，后面研究)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3e72f-1aa0-4509-9f6e-6d942f249195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class HuggingfaceModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = self.model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface\"\n",
    "\n",
    "    def generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> List[str]:\n",
    "        callback_manager = CallbackManagerForLLMRun.get_instance()\n",
    "        with callback_manager.as_context():\n",
    "            callback_manager.on_llm_start({self._llm_type: len(prompts)})\n",
    "            results = [self._call(prompt, stop) for prompt in prompts]\n",
    "            callback_manager.on_llm_end({self._llm_type: len(prompts)})\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3bd31-bd9d-4189-8213-a9b64bdf4ffa",
   "metadata": {},
   "source": [
    "# 下面这个是没问题的  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c424c9-a704-42cf-bc82-64a1630c0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "class ChatGLM_LLM(LLM):\n",
    "    # 基于本地 InternLM 自定义 LLM 类\n",
    "    tokenizer : AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: InternLM 模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(torch.bfloat16).cuda()\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        # 重写调用函数\n",
    "        response, history = self.model.chat(self.tokenizer, prompt , history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM3-6B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b244104-b7ef-49ea-a2fa-ea876eaf6987",
   "metadata": {},
   "source": [
    "# langchain 集成  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4716c927-9cf6-4cb7-b163-e35adce9571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6d064da3f04ae3b606dca73a533c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "# 加载文件夹中的所有txt类型的文件\n",
    "loader = DirectoryLoader(r'D:/Notes/NLP review', glob='**/*.md')\n",
    "# 将数据转成 document 对象，每个文件会作为一个 document\n",
    "documents = loader.load()\n",
    "\n",
    "# 初始化加载器\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "# 切割加载的 document\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 指定 Hugging Face 的预训练模型名称\n",
    "model_name = r\"D:\\CodeLibrary\\ChatGLM\\embedtext2vec\"  # 示例模型名称，您可以根据需要更改\n",
    "\n",
    "# 创建 HuggingFaceEmbeddings 对象\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# 将 document 通过 HuggingFace 的 embeddings 对象计算 embedding 向量信息并临时存入 Chroma 向量数据库，用于后续匹配查询\n",
    "docsearch = Chroma.from_documents(split_docs, embeddings)\n",
    "\n",
    "\n",
    "model_name = \"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "hf_model = ChatGLM_LLM(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211d00f-c420-44a1-9201-628f9f22cbed",
   "metadata": {},
   "source": [
    "# 参考代码 \n",
    "```python\n",
    "store = Chroma.load(\"chroma_store\", embeddings)\n",
    "\n",
    "template = \"\"\"基于以下信息来回答用户问题。                          \n",
    "已知信息： \n",
    "{context} \n",
    "问题：\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "qa = RetrievalQA.from_chain_type(llm=chatglm, retriever=store.as_retriever(), chain_type=\"stuff\",\n",
    "                                chain_type_kwargs=chain_type_kwargs, return_source_documents=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49d0206-57fd-4625-be01-ff917e7ed3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '电影《红高粱》简介？', 'result': '电影《红高粱》是一部由张艺谋执导的彩色故事片，于 1987 年 10 月 23 日在北京颐和园陶然亭畔放映。该电影改编自莫言的同名小说，讲述了 20 世纪 50 年代至 70 年代发生在一个中国北方小村庄中的故事。\\n\\n故事的主人公是充满生命力和热情的女性角色九儿（巩俐饰演），她是一个坚韧不拔、充满生命力的女性，她在困境中努力生存，并在家庭和社会中扮演着重要的角色。\\n\\n电影讲述了九儿和她身边的几个重要人物之间的复杂关系和情感纠葛，其中包括了她的丈夫、情人、兄弟和邻居等。这些人物在不同的历史背景下经历了不同的命运，而九儿则以其强大的意志力和勇气，不断地面对困境并保护自己和家人。\\n\\n《红高粱》以其深刻的社会意义、生动的人物形象和出色的表演，赢得了广泛的关注和赞誉，成为了一部经典的中国电影。', 'source_documents': [Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'}), Document(page_content='精确率（precision）和召回率（recall）关系\\n\\n常见一些问题：\\n\\n为什么精确率越高，召回率越低？\\n\\n在二分类问题中，精确度（Precision）和召回率（Recall）是两个常用的性能指标，用于评估分类模型的表现。它们分别衡量了分类器的两个不同方面：\\n\\n精确度（Precision）：\\n\\n精确度是指在所有被分类为正类别的样本中，正确被分类为正类别的样本所占的比例。即在所有预测为正类别的样本中，正确预测为正类别的样本的比例。\\n\\n精确度高表示模型预测为正类别的样本中，真正属于正类别的比例较高。\\n\\n召回率（Recall）：\\n\\n召回率是指在所有真实的正类别样本中，被正确预测为正类别的样本所占的比例。即在所有实际为正类别的样本中，被正确预测为正类别的样本的比例。\\n\\n召回率高表示模型能够正确识别出较多的正类别样本，即模型对正类别的覆盖能力较强。\\n\\n理解了精确度和召回率的定义后，我们可以探讨为什么在某些情况下，精确度越高，召回率越低的现象可能会出现：\\n\\n高精确度，低召回率的情况：\\n\\n当模型更加谨慎地将样本分类为正类别时，可能会提高精确度。这意味着模型更加保守，只在非常确信的情况下将样本预测为正类别，从而减少了误报的可能性。\\n\\n然而，这种保守性可能会导致模型漏掉一些真实的正类别样本，从而降低了召回率。模型更倾向于避免将负类别样本错误预测为正类别，但同时可能错过了一些真正的正类别样本。\\n\\n应对不平衡数据：\\n\\n当数据集中正类别样本相对较少，而负类别样本相对较多时，模型可能更倾向于对负类别样本进行分类，因为这样可以降低错误分类的风险，提高整体精确度。\\n\\n但是，由于正类别样本较少，模型可能会错过一些真正的正类别样本，导致召回率较低。\\n\\n综上所述，高精确度和低召回率的现象可能源自模型对样本分类的保守性，尤其是在面对不平衡数据集时。在实际应用中，需要根据具体的问题和需求，权衡精确度和召回率，并选择合适的调节策略以达到最佳的性能。', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\machine learning\\\\ml_matrices.md'}), Document(page_content='routing （路由）\\n\\nlangchain 官网英文文档介绍的 routing\\n\\n有时我们有针对不同领域的多个索引，并且对于不同的问题，我们希望查询这些索引的不同子集。例如，假设我们有一个用于所有 LangChain python 文档的向量存储索引，以及一个用于所有 LangChain js 文档的向量存储索引。给定一个有关 LangChain 使用的问题，我们想要推断问题所指的是哪种语言并查询相应的文档。Query routing是对应该对哪个索引或索引子集执行查询进行分类的过程。\\n\\n逻辑路由（logic routing）\\n\\n逻辑路由更侧重于查询的结构和形式，它根据查询的逻辑结构来确定处理路径。\\n\\n在RAG系统中，逻辑路由可能涉及到对查询语句的解析，识别出查询中的逻辑关系，如AND、OR、NOT等操作。\\n\\n逻辑路由的目标是确保查询按照既定的规则和顺序被正确处理，以便系统能够生成准确的回答。\\n\\n例如，如果用户提出一个复杂的SQL查询，逻辑路由会确保查询中的各个部分按照正确的逻辑顺序被执行。\\n\\n语义路由（semantic routing）\\n\\n语义路由侧重于理解查询的语义内容，即查询的实际含义和意图。\\n\\n在RAG系统中，语义路由可能涉及到自然语言处理（NLP）技术，如语义分析、实体识别和意图识别，以确保系统能够准确理解用户的问题。\\n\\n语义路由的目标是将用户的查询映射到最相关的数据源或处理路径，即使这些路径可能不是直观的或直接的。\\n\\n例如，如果用户询问“最近的天气如何？”，语义路由会识别出这是一个关于天气查询的问题，并将其路由到提供天气信息的服务或数据库。\\n\\n总结\\n\\nlogic routing是解决可能有不同的datasource，semantic routing是解决可能有不同的prompt。动态解决这些问题，就需要routing。', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\routing\\\\routing.md'}), Document(page_content='LoRA 涉及到的一些数学知识等\\n\\n用LoRA fine-tuning 能够用少量参数媲美全参数的效果。（非常重要的一个fine-tuned技术）\\n\\nLoRA核心，rank decomposition matrices\\n\\n==（这是LoRA为什么可以计算量小的重要原因）==\\n\\n秩分解和特征分解的区别\\n\\n==（也就是说，rank decomposition 跟 常见的特征分解不是一个东西！线代基础）==\\n\\n将矩阵分解为特征值和特征矩阵通常被称为特征分解（Eigenvalue Decomposition），而不是秩分解（Rank Decomposition）。特征分解是一种特定的矩阵分解方法，它将一个方阵（即行数和列数相等的矩阵）分解为特征值和特征向量的乘积。\\n对于一个方阵 ( A )，如果它有 ( n ) 个线性无关的特征向量 ( v_1, v_2, ..., v_n ) 和对应的特征值 ( \\\\lambda_1, \\\\lambda_2, ..., \\\\lambda_n )，那么特征分解可以表示为：\\n[ A = V \\\\Lambda V^{-1} ]\\n其中，( V ) 是由特征向量构成的矩阵，( \\\\Lambda ) 是对角矩阵，其对角线上的元素是特征值 ( \\\\lambda_1, \\\\lambda_2, ..., \\\\lambda_n )。\\n\\n特征分解的主要目的是为了简化矩阵的运算，例如计算矩阵的幂、求解线性微分方程等。特征分解揭示了矩阵的内在结构，比如矩阵的秩等于其非零特征值的个数。\\n\\n虽然特征分解和秩分解都是矩阵分解的方法，但它们的目的和应用场景不同。秩分解通常用于降低矩阵的维度或近似矩阵，而特征分解用于分析矩阵的性质和结构。在秩分解中，我们通常关注的是如何用低秩的矩阵来近似原始矩阵，而在特征分解中，我们关注的是矩阵的特征值和特征向量。\\n\\n又涉及到主成分分析（PCA）和奇异值分解（SVD）\\n\\n目的就是降维，因为 LoRA 就是低秩分解的过程。\\n\\nLoRA 的一些结论\\n\\n初始化矩阵B和矩阵A时，B初始化为0，A进行高斯分布的随机初始化。\\n\\n选择低秩矩阵时，选择多个待调矩阵并提供 较小r 的策略比选择单一矩阵并提供 较大r 的策略好。\\n\\n首先在r的选择上，取r=1已经表现足够优秀。(作者说的)\\n\\n一些问题\\n\\n为什么B要初始化为0呢？\\n\\n在使用LoRA进行微调时，选择将矩阵 B 初始化为零，而将矩阵 A 进行高斯分布的随机初始化，可能是出于以下几个方面的考虑：\\n\\n简化模型参数空间： 将矩阵 B 初始化为零可以简化模型的参数空间，并且在开始微调时不会对模型产生过多的影响。这样做可以使得模型更容易收敛到一个合适的解。\\n\\n控制参数数量： 将矩阵 B 初始化为零可以有效地减少模型参数的数量，从而降低计算成本和内存占用。这在资源受限的情况下尤其有用。\\n\\n避免过拟合： 零初始化可以减少过拟合的风险。如果矩阵 B 的参数过多且过大，可能会导致模型在微调时对训练数据过度拟合，而零初始化可以一定程度上减轻这种情况。\\n\\n降低初始误差： 零初始化可以避免在微调初始阶段引入额外的误差。如果 B 初始化为非零值，可能会引入一定程度的初始偏差，从而影响模型的收敛速度和性能。\\n\\n总的来说，将矩阵 B 初始化为零可能是为了简化模型、降低计算成本、减少过拟合的风险以及避免初始误差等方面考虑的结果。', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\fine-tuned\\\\LoRA\\\\LoRA related knowledge.md'})]}\n"
     ]
    }
   ],
   "source": [
    "# 创建问答对象\n",
    "# qa = RetrievalQA.from_chain_type(llm=hf_model, chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=hf_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 进行问答 Chain          \n",
    "\"\"\"\n",
    "    类的 __call__ 方法已经被弃用， 用 invoke 方法来代替 __call__ 方法    \n",
    "\"\"\"\n",
    "# result = qa({\"query\": \"电影《红高粱》简介？\"})\n",
    "# print(result)\n",
    "\n",
    "result = qa.invoke({\"query\": \"电影《红高粱》简介？\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b6ea12-4847-4147-a737-1b8940a978ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query', 'result', 'source_documents'])\n"
     ]
    }
   ],
   "source": [
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b86b01-420d-4a06-9227-097d532af44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电影《红高粱》简介？\n"
     ]
    }
   ],
   "source": [
    "print(result['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1937d4b7-0bef-43e2-af5e-86a3523c47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电影《红高粱》是一部由张艺谋执导的彩色故事片，于 1987 年 10 月 23 日在北京颐和园陶然亭畔放映。该电影改编自莫言的同名小说，讲述了 20 世纪 50 年代至 70 年代发生在一个中国北方小村庄中的故事。\n",
      "\n",
      "故事的主人公是充满生命力和热情的女性角色九儿（巩俐饰演），她是一个坚韧不拔、充满生命力的女性，她在困境中努力生存，并在家庭和社会中扮演着重要的角色。\n",
      "\n",
      "电影讲述了九儿和她身边的几个重要人物之间的复杂关系和情感纠葛，其中包括了她的丈夫、情人、兄弟和邻居等。这些人物在不同的历史背景下经历了不同的命运，而九儿则以其强大的意志力和勇气，不断地面对困境并保护自己和家人。\n",
      "\n",
      "《红高粱》以其深刻的社会意义、生动的人物形象和出色的表演，赢得了广泛的关注和赞誉，成为了一部经典的中国电影。\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752939f6-515e-4052-a49a-ab3d487a4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='RAG Fusion\\n\\nRAG Fusion 参考，还讲了 RRF算法\\n\\n概念\\n\\nRAG 融合（RAG Fusion）是指在 RAG 模型中将检索和生成两个阶段进行有效整合的过程。在传统的 RAG 模型中，首先进行文档检索以获取相关文档，然后使用生成模型根据这些文档生成答案。但在某些情况下，这种简单的串行方法可能无法充分利用检索和生成之间的互补性。\\n\\nRAG 融合的目的是在保留检索和生成各自优势的同时，更加有效地利用两者之间的关系，从而提高整体的性能。这可以通过以下几种方式来实现：\\n\\n交互式融合：在检索和生成之间建立一种交互式的机制，让它们可以相互影响和调整。例如，在生成阶段可以考虑将生成的内容作为反馈信息反过来影响检索过程，以提高检索的准确性。\\n\\n信息传递：在检索到相关文档后，将一些关键信息传递给生成模型，以帮助生成更加相关和准确的答案。这些信息可以是关键词、上下文信息等。\\n\\n多阶段生成：将生成过程分为多个阶段，每个阶段都与检索结果有关，逐步细化生成的内容。例如，先生成一个粗略的答案，然后根据检索到的文档进一步完善和细化答案。\\n\\n端到端训练：将检索和生成过程作为一个整体进行端到端的训练，以更好地优化两者之间的关系，使得模型能够更好地学习到检索和生成之间的互相影响。\\n\\n通过 RAG 融合，可以更好地整合检索和生成两个阶段，充分发挥它们各自的优势，从而提高整体系统的性能和效果。\\n\\n流程图部分\\n\\n之前的 多重查询之后， 可能会存在很多相似文档，给大模型之前肯定还是需要做些处理，故上面流程图中，文档与大模型之间的部分，就是 Fusion。这里涉及到一个 算法(Reciprocal ran fusion)，这是一个对文档进行打分的一个算法\\n\\nreciprocal rank fusion\\n\\n案例流程图\\n\\n计算公式\\n$$RRFscore(d\\\\in D)=\\\\sum_{r\\\\in R}\\\\frac1{k+r(d)} ,$$\\n\\n来源（https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf）', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\rag Fusion\\\\rag fusion.md'}), Document(page_content='精确率（precision）和召回率（recall）关系\\n\\n常见一些问题：\\n\\n为什么精确率越高，召回率越低？\\n\\n在二分类问题中，精确度（Precision）和召回率（Recall）是两个常用的性能指标，用于评估分类模型的表现。它们分别衡量了分类器的两个不同方面：\\n\\n精确度（Precision）：\\n\\n精确度是指在所有被分类为正类别的样本中，正确被分类为正类别的样本所占的比例。即在所有预测为正类别的样本中，正确预测为正类别的样本的比例。\\n\\n精确度高表示模型预测为正类别的样本中，真正属于正类别的比例较高。\\n\\n召回率（Recall）：\\n\\n召回率是指在所有真实的正类别样本中，被正确预测为正类别的样本所占的比例。即在所有实际为正类别的样本中，被正确预测为正类别的样本的比例。\\n\\n召回率高表示模型能够正确识别出较多的正类别样本，即模型对正类别的覆盖能力较强。\\n\\n理解了精确度和召回率的定义后，我们可以探讨为什么在某些情况下，精确度越高，召回率越低的现象可能会出现：\\n\\n高精确度，低召回率的情况：\\n\\n当模型更加谨慎地将样本分类为正类别时，可能会提高精确度。这意味着模型更加保守，只在非常确信的情况下将样本预测为正类别，从而减少了误报的可能性。\\n\\n然而，这种保守性可能会导致模型漏掉一些真实的正类别样本，从而降低了召回率。模型更倾向于避免将负类别样本错误预测为正类别，但同时可能错过了一些真正的正类别样本。\\n\\n应对不平衡数据：\\n\\n当数据集中正类别样本相对较少，而负类别样本相对较多时，模型可能更倾向于对负类别样本进行分类，因为这样可以降低错误分类的风险，提高整体精确度。\\n\\n但是，由于正类别样本较少，模型可能会错过一些真正的正类别样本，导致召回率较低。\\n\\n综上所述，高精确度和低召回率的现象可能源自模型对样本分类的保守性，尤其是在面对不平衡数据集时。在实际应用中，需要根据具体的问题和需求，权衡精确度和召回率，并选择合适的调节策略以达到最佳的性能。', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\machine learning\\\\ml_matrices.md'}), Document(page_content='routing （路由）\\n\\nlangchain 官网英文文档介绍的 routing\\n\\n有时我们有针对不同领域的多个索引，并且对于不同的问题，我们希望查询这些索引的不同子集。例如，假设我们有一个用于所有 LangChain python 文档的向量存储索引，以及一个用于所有 LangChain js 文档的向量存储索引。给定一个有关 LangChain 使用的问题，我们想要推断问题所指的是哪种语言并查询相应的文档。Query routing是对应该对哪个索引或索引子集执行查询进行分类的过程。\\n\\n逻辑路由（logic routing）\\n\\n逻辑路由更侧重于查询的结构和形式，它根据查询的逻辑结构来确定处理路径。\\n\\n在RAG系统中，逻辑路由可能涉及到对查询语句的解析，识别出查询中的逻辑关系，如AND、OR、NOT等操作。\\n\\n逻辑路由的目标是确保查询按照既定的规则和顺序被正确处理，以便系统能够生成准确的回答。\\n\\n例如，如果用户提出一个复杂的SQL查询，逻辑路由会确保查询中的各个部分按照正确的逻辑顺序被执行。\\n\\n语义路由（semantic routing）\\n\\n语义路由侧重于理解查询的语义内容，即查询的实际含义和意图。\\n\\n在RAG系统中，语义路由可能涉及到自然语言处理（NLP）技术，如语义分析、实体识别和意图识别，以确保系统能够准确理解用户的问题。\\n\\n语义路由的目标是将用户的查询映射到最相关的数据源或处理路径，即使这些路径可能不是直观的或直接的。\\n\\n例如，如果用户询问“最近的天气如何？”，语义路由会识别出这是一个关于天气查询的问题，并将其路由到提供天气信息的服务或数据库。\\n\\n总结\\n\\nlogic routing是解决可能有不同的datasource，semantic routing是解决可能有不同的prompt。动态解决这些问题，就需要routing。', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\RAG related\\\\RAG base\\\\routing\\\\routing.md'}), Document(page_content='LoRA 涉及到的一些数学知识等\\n\\n用LoRA fine-tuning 能够用少量参数媲美全参数的效果。（非常重要的一个fine-tuned技术）\\n\\nLoRA核心，rank decomposition matrices\\n\\n==（这是LoRA为什么可以计算量小的重要原因）==\\n\\n秩分解和特征分解的区别\\n\\n==（也就是说，rank decomposition 跟 常见的特征分解不是一个东西！线代基础）==\\n\\n将矩阵分解为特征值和特征矩阵通常被称为特征分解（Eigenvalue Decomposition），而不是秩分解（Rank Decomposition）。特征分解是一种特定的矩阵分解方法，它将一个方阵（即行数和列数相等的矩阵）分解为特征值和特征向量的乘积。\\n对于一个方阵 ( A )，如果它有 ( n ) 个线性无关的特征向量 ( v_1, v_2, ..., v_n ) 和对应的特征值 ( \\\\lambda_1, \\\\lambda_2, ..., \\\\lambda_n )，那么特征分解可以表示为：\\n[ A = V \\\\Lambda V^{-1} ]\\n其中，( V ) 是由特征向量构成的矩阵，( \\\\Lambda ) 是对角矩阵，其对角线上的元素是特征值 ( \\\\lambda_1, \\\\lambda_2, ..., \\\\lambda_n )。\\n\\n特征分解的主要目的是为了简化矩阵的运算，例如计算矩阵的幂、求解线性微分方程等。特征分解揭示了矩阵的内在结构，比如矩阵的秩等于其非零特征值的个数。\\n\\n虽然特征分解和秩分解都是矩阵分解的方法，但它们的目的和应用场景不同。秩分解通常用于降低矩阵的维度或近似矩阵，而特征分解用于分析矩阵的性质和结构。在秩分解中，我们通常关注的是如何用低秩的矩阵来近似原始矩阵，而在特征分解中，我们关注的是矩阵的特征值和特征向量。\\n\\n又涉及到主成分分析（PCA）和奇异值分解（SVD）\\n\\n目的就是降维，因为 LoRA 就是低秩分解的过程。\\n\\nLoRA 的一些结论\\n\\n初始化矩阵B和矩阵A时，B初始化为0，A进行高斯分布的随机初始化。\\n\\n选择低秩矩阵时，选择多个待调矩阵并提供 较小r 的策略比选择单一矩阵并提供 较大r 的策略好。\\n\\n首先在r的选择上，取r=1已经表现足够优秀。(作者说的)\\n\\n一些问题\\n\\n为什么B要初始化为0呢？\\n\\n在使用LoRA进行微调时，选择将矩阵 B 初始化为零，而将矩阵 A 进行高斯分布的随机初始化，可能是出于以下几个方面的考虑：\\n\\n简化模型参数空间： 将矩阵 B 初始化为零可以简化模型的参数空间，并且在开始微调时不会对模型产生过多的影响。这样做可以使得模型更容易收敛到一个合适的解。\\n\\n控制参数数量： 将矩阵 B 初始化为零可以有效地减少模型参数的数量，从而降低计算成本和内存占用。这在资源受限的情况下尤其有用。\\n\\n避免过拟合： 零初始化可以减少过拟合的风险。如果矩阵 B 的参数过多且过大，可能会导致模型在微调时对训练数据过度拟合，而零初始化可以一定程度上减轻这种情况。\\n\\n降低初始误差： 零初始化可以避免在微调初始阶段引入额外的误差。如果 B 初始化为非零值，可能会引入一定程度的初始偏差，从而影响模型的收敛速度和性能。\\n\\n总的来说，将矩阵 B 初始化为零可能是为了简化模型、降低计算成本、减少过拟合的风险以及避免初始误差等方面考虑的结果。', metadata={'source': 'D:\\\\Notes\\\\NLP review\\\\fine-tuned\\\\LoRA\\\\LoRA related knowledge.md'})]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9cc4c83-2570-46f5-8d38-664c3131f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @classmethod\n",
      "    def from_chain_type(\n",
      "        cls,\n",
      "        llm: BaseLanguageModel,\n",
      "        chain_type: str = \"stuff\",\n",
      "        chain_type_kwargs: Optional[dict] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> BaseRetrievalQA:\n",
      "        \"\"\"Load chain from chain type.\"\"\"\n",
      "        _chain_type_kwargs = chain_type_kwargs or {}\n",
      "        combine_documents_chain = load_qa_chain(\n",
      "            llm, chain_type=chain_type, **_chain_type_kwargs\n",
      "        )\n",
      "        return cls(combine_documents_chain=combine_documents_chain, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(RetrievalQA.from_chain_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b48d08-90eb-4930-9655-ae133a527bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Replacement Token Detection) 是一种预训练任务，旨在通过对输入句子中的某些token进行替换，并要求模型预测哪些token被替换了来进行模型训练。它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。然后模型再训练一个判别器，来预测每个词是原始词还是替换词。RTD任务的目的是学习区分输入的词，它不使用掩码，而是从一个建议分布中采样词来替换输入，这解决了掩码带来的预训练和 fine-tune 不一致的问题。然后模型再训练一个判别器，来预测每个词是原始词还是替换词。\n"
     ]
    }
   ],
   "source": [
    "a = qa.invoke({'query':'什么是RAG'})\n",
    "print(a['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a96707-2f23-4ae2-9cfc-918af612dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LLM(BaseLLM):\n",
      "    \"\"\"Simple interface for implementing a custom LLM.\n",
      "\n",
      "    You should subclass this class and implement the following:\n",
      "\n",
      "    - `_call` method: Run the LLM on the given prompt and input (used by `invoke`).\n",
      "    - `_identifying_params` property: Return a dictionary of the identifying parameters\n",
      "        This is critical for caching and tracing purposes. Identifying parameters\n",
      "        is a dict that identifies the LLM.\n",
      "        It should mostly include a `model_name`.\n",
      "\n",
      "    Optional: Override the following methods to provide more optimizations:\n",
      "\n",
      "    - `_acall`: Provide a native async version of the `_call` method.\n",
      "        If not provided, will delegate to the synchronous version using\n",
      "        `run_in_executor`. (Used by `ainvoke`).\n",
      "    - `_stream`: Stream the LLM on the given prompt and input.\n",
      "        `stream` will use `_stream` if provided, otherwise it\n",
      "        use `_call` and output will arrive in one chunk.\n",
      "    - `_astream`: Override to provide a native async version of the `_stream` method.\n",
      "        `astream` will use `_astream` if provided, otherwise it will implement\n",
      "        a fallback behavior that will use `_stream` if `_stream` is implemented,\n",
      "        and use `_acall` if `_stream` is not implemented.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def _call(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        stop: Optional[List[str]] = None,\n",
      "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> str:\n",
      "        \"\"\"Run the LLM on the given input.\n",
      "\n",
      "        Override this method to implement the LLM logic.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to generate from.\n",
      "            stop: Stop words to use when generating. Model output is cut off at the\n",
      "                first occurrence of any of the stop substrings.\n",
      "                If stop tokens are not supported consider raising NotImplementedError.\n",
      "            run_manager: Callback manager for the run.\n",
      "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "                to the model provider API call.\n",
      "\n",
      "        Returns:\n",
      "            The model output as a string. SHOULD NOT include the prompt.\n",
      "        \"\"\"\n",
      "\n",
      "    async def _acall(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        stop: Optional[List[str]] = None,\n",
      "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> str:\n",
      "        \"\"\"Async version of the _call method.\n",
      "\n",
      "        The default implementation delegates to the synchronous _call method using\n",
      "        `run_in_executor`. Subclasses that need to provide a true async implementation\n",
      "        should override this method to reduce the overhead of using `run_in_executor`.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to generate from.\n",
      "            stop: Stop words to use when generating. Model output is cut off at the\n",
      "                first occurrence of any of the stop substrings.\n",
      "                If stop tokens are not supported consider raising NotImplementedError.\n",
      "            run_manager: Callback manager for the run.\n",
      "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "                to the model provider API call.\n",
      "\n",
      "        Returns:\n",
      "            The model output as a string. SHOULD NOT include the prompt.\n",
      "        \"\"\"\n",
      "        return await run_in_executor(\n",
      "            None,\n",
      "            self._call,\n",
      "            prompt,\n",
      "            stop,\n",
      "            run_manager.get_sync() if run_manager else None,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "    def _generate(\n",
      "        self,\n",
      "        prompts: List[str],\n",
      "        stop: Optional[List[str]] = None,\n",
      "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> LLMResult:\n",
      "        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n",
      "        # TODO: add caching here.\n",
      "        generations = []\n",
      "        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\n",
      "        for prompt in prompts:\n",
      "            text = (\n",
      "                self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "                if new_arg_supported\n",
      "                else self._call(prompt, stop=stop, **kwargs)\n",
      "            )\n",
      "            generations.append([Generation(text=text)])\n",
      "        return LLMResult(generations=generations)\n",
      "\n",
      "    async def _agenerate(\n",
      "        self,\n",
      "        prompts: List[str],\n",
      "        stop: Optional[List[str]] = None,\n",
      "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> LLMResult:\n",
      "        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n",
      "        generations = []\n",
      "        new_arg_supported = inspect.signature(self._acall).parameters.get(\"run_manager\")\n",
      "        for prompt in prompts:\n",
      "            text = (\n",
      "                await self._acall(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "                if new_arg_supported\n",
      "                else await self._acall(prompt, stop=stop, **kwargs)\n",
      "            )\n",
      "            generations.append([Generation(text=text)])\n",
      "        return LLMResult(generations=generations)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "import inspect\n",
    "\n",
    "print(inspect.getsource(LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952977fd-e7dc-4ce9-9e45-2a9f818d69da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
